## Abstract
在低光图像增强中，许多深度学习算法都基于Retinex理论。然而，传统的Retinex模型未考虑到暗处隐藏的污染或由光照过程引入的污染。此外，这些方法通常需要繁琐的多阶段训练流程，并且依赖卷积神经网络（CNN），在捕捉长程依赖关系方面存在局限性。本文提出了一个简单而有原则的**单阶段Retinex框架（ORF）**。ORF首先估计光照信息来增强低光图像，然后恢复污染以生成增强后的图像。我们设计了一个**光照引导的Transformer（IGT）**，利用光照表示来指导不同光照条件区域的非局部交互建模。**通过将IGT嵌入到ORF中，我们得到了我们的算法——Retinexformer**。综合的定量和定性实验表明，Retinexformer在13个基准测试中显著超过了最先进的方法。用户研究和低光目标检测的应用也揭示了我们方法的潜在实际价值。代码可在[https://github.com/caiyuanhao1998/Retinexformer](https://github.com/caiyuanhao1998/Retinexformer)获取。

---
## Introduction
低光图像增强是计算机视觉中的一个重要且具有挑战性的任务。它旨在改善低光图像的可视性和对比度，并恢复暗处或光照过程引入的污染（例如噪声、伪影、色彩失真等）。这些问题不仅挑战人类的视觉感知，也挑战其他视觉任务，如夜间物体检测。

因此，已经提出了大量的算法用于低光图像增强。然而，这些现有的算法都有各自的缺点。像**直方图均衡化**和**伽马校正**等简单方法直接放大低可见度和对比度的图像，但它们**几乎没有考虑光照因素**，因此常常产生不必要的伪影。传统的认知方法依赖于***Retinex理论***，该理论假设 *颜色图像可以分解为反射率和光照* 两个组件。与简单方法不同，传统方法专注于光照估计，但由于这些方法假设图像没有噪声或色彩失真，因此常常会引入严重的噪声或局部颜色失真，这与真实的低光场景不一致。

随着深度学习的快速发展，**卷积神经网络（CNN）** 已被广泛应用于低光图像增强。比如，Wei等人及后续工作将Retinex分解与深度学习相结合。然而，这些方法通常**面临繁琐的多阶段训练流程**。它们采用不同的CNN来分别分解颜色图像、去噪反射率并调整光照。需要*首先独立训练这些CNN，然后将它们连接在一起进行端到端微调*。训练过程繁琐且耗时。

此外，这些基于CNN的方法在捕捉**长程依赖性**和**非局部自相似性**方面也存在局限性，而这些特性对于图像恢复至关重要。近年来崛起的深度学习模型——Transformer，可能为解决这些问题提供了可能性。然而，直接将**原始视觉Transformer**应用于低光图像增强可能会遇到一个问题：**计算复杂度与输入空间的大小成二次方关系。这种计算成本可能是难以承受的。** 由于这个限制，一些混合CNN-Transformer的算法（如SNR-Net）仅在U型CNN的最低空间分辨率上使用单个全局Transformer层。因此，Transformer在低光图像增强中的潜力仍未得到充分探索。

为了应对上述问题，我们提出了一种新方法——Retinexformer，用于低光图像增强。首先，我们提出了一个简单而有原则的**单阶段Retinex框架（ORF）**。通过引入扰动项来修正反射率和光照，我们修订了原始的Retinex模型。
***我们的ORF首先估计光照信息，并利用它来增强低光图像。接着，ORF通过使用一个失真恢复模块来压制噪声、伪影、曝光不足/过度以及色彩失真。与之前的Retinex深度学习框架不同，这些框架通常需要多阶段训练流程，我们的ORF采用单阶段训练方式，简化了训练过程。其次，我们设计了一个光照引导的Transformer（IGT），用于建模长程依赖性。IGT的关键组件是光照引导的多头自注意力（IG-MSA）。IG-MSA利用光照表示来指导自注意力的计算，并增强不同曝光条件区域之间的交互。最后，我们将IGT嵌入到ORF中作为失真恢复器，从而得到了我们的Retinexformer方法。*** 
如图1所示，Retinexformer在多个数据集上显著超越了最先进的方法。特别是在SID、SDSD-室内和LOL-v2-合成数据集上，性能提高了超过6 dB。

我们的贡献可以总结如下：

- 我们提出了**第一个基于Transformer的低光图像增强算法——Retinexformer**。
- 我们提出了一个**单阶段的Retinex框架（ORF）**，其训练过程简单且能够有效建模污染。
- 我们设计了一种新的自注意力机制——**IG-MSA**，利用光照信息作为关键线索，引导长程依赖建模。
- 定量和定性实验表明，Retinexformer在13个数据集上超过了最先进的方法。用户研究和低光目标检测的结果也证明了我们方法的潜在实际价值。

---
## Related Works
### 低光图像增强

- **简单方法**：简单方法，如直方图均衡化和伽马校正（Gamma Correction，GC），直接放大低曝光图像的可见度和对比度。然而，**这些方法几乎不考虑照明因素，使得增强后的图像在感知上与实际正常光照场景不一致。**
- **传统认知方法**：与简单算法不同，传统方法考虑了照明因素。它们依赖于**Retinex理论**，并将低光图像的反射分量视为增强结果的合理解。举例来说，Guo等人提出通过对初始估算的照明图施加结构先验来优化它。然而，这些方法**天真地假设低光图像没有污染**，导致在增强过程中**产生严重的噪声和颜色失真**。此外，这些方法**依赖于手工设计的先验**，通常需要仔细的参数调整，并且**在泛化能力上表现较差**。
- **深度学习方法**：随着深度学习的快速发展，**卷积神经网络（CNN）** 被广泛应用于低光图像增强。例如，Wei等人及其后续工作将Retinex分解与深度学习相结合。然而，这些方法通常**面临繁琐的多阶段训练流程**。几种CNN被用来分别学习或调整Retinex模型的不同组件。**Wang等人提出了一种一阶段的Retinex基础CNN，名为DeepUPE**，直接预测照明图。然而，**DeepUPE并未考虑到污染因素**，导致在放大低曝光图像时产生噪声和颜色失真。此外，这些**基于CNN的方法在捕捉不同区域的长程依赖关系时也表现出局限性。**

### 视觉Transformer

近年来，Transformer及其变体已被应用于许多计算机视觉任务。例如，Xu等人提出了一种SNR感知的CNN-Transformer混合网络SNR-Net，用于低光图像增强。然而，由于**全局多头自注意力（MSA）的巨大计算开销**，SNR-Net仅在*U形CNN的最低分辨率上使用单个全局Transformer层*。**Transformer在低光图像增强中的潜力尚未得到充分发挥。**

---
## Method

下图展示了我们方法的总体架构。  
![[Pasted image 20251018092340.png]]
图 2(a) ：
- Retinexformer 基于单阶段 Retinex 框架（One-stage Retinex-based Framework, ORF）。  
- ORF 由一个**照明估计器（illumination estimator）**(i) 和一个**失真恢复器（corruption restorer）**(ii) 组成。 
- 我们设计了一个**照明引导 Transformer（Illumination-Guided Transformer, IGT）** 来充当失真恢复器的角色。  

图 2(b) ：
- IGT 的基本单元是**照明引导注意力块（Illumination-Guided Attention Block, IGAB）**，它由两个层归一化（LN）、一个**照明引导多头自注意力（Illumination-Guided Multi-head Self-Attention, IG-MSA）** 模块和一个前馈网络（FFN）组成。  图 2(c) 展示了 IG-MSA 的细节。
------
### 3.1 单阶段 Retinex 框架（One-stage Retinex-based Framework, ORF）

根据 Retinex 理论，一个低光图像 $I ∈ R^{H×W×3}$ 可以被分解为一个反射率图像 $R ∈ R^{H×W×3}$ 和一个照明图 $L ∈ R^{H×W}$，其关系为：
$$
I = R ⊙ L\tag{1}
$$
其中 $⊙$ 表示逐元素乘法。
>就是Retinex理论的基本假设。

 该 Retinex 模型假设 $I$ 是无损坏（corruption-free）的，这与真实的低曝光场景不一致。
 我们分析得出，损坏主要来源于两个因素：
 首先，暗场景的高 ISO 和长曝光拍摄设置不可避免地会引入噪声和伪影；
 其次，**亮化过程（light-up process）** 可能会**放大噪声和伪影**，同时还会造成**欠曝光/过曝光以及颜色失真**，如图 2(a) 中放大的局部区域 i 和 ii 所示。

为了建模这些损坏，我们通过在 $R$ 和 $L$ 中分别引入扰动项，重新公式化公式 (1)：
$$
I = (R + \hat{R}) ⊙ (L + \hat{L})
= R ⊙ L + R ⊙ \hat{L} + \hat{R} ⊙ (L + \hat{L}) \tag{2}
$$
其中 $\hat{R} ∈ R^{H×W×3}$ 和 $\hat{L} ∈ R^{H×W}$ 表示扰动项。我们将 $R$ 视为一个良好曝光的图像。
为了将 $I$ 亮化，我们对公式 (2) 的两边逐元素相乘一个亮化映射 $\bar{L}$，使得 $\bar{L} ⊙ L = 1$，得到：
$$
I ⊙ \bar{L} = R + R ⊙ (\hat{L} ⊙ \bar{L}) + (\hat{R} ⊙ (L + \hat{L})) ⊙ \bar{L}, \tag{3}
$$
> $\bar{L} ⊙ L = 1$的意思是，亮化映射和原光照图的每个元素相乘乘积都是1，这样可以修正一些过大/过小的光照点。

其中我们将后面两项看成损坏项。可以如此看待：
- $\hat{R} ⊙ (L + \hat{L})$ 表示隐藏在暗场景中的噪声和伪影，并且会被 $\bar{L}$ 放大；
- $R ⊙ (\hat{L} ⊙ \bar{L})$ 表示由亮化过程引起的欠曝光/过曝光及颜色失真。
我们将公式 (3) 简化为：
$$
I_{lu} = I ⊙ \bar{L} = R + C, \tag{4}
$$
其中 $I_{lu} ∈ R^{H×W×3}$ 表示亮化后的图像，$C ∈ R^{H×W×3}$ 表示总体损坏项。
随后，我们将 ORF 表述为：
$$
(I_{lu}, F_{lu}) = \mathcal E(I, L_p), \quad I_{en} = \mathcal R(I_{lu}, F_{lu}), \tag{5}
$$
其中 $E$ 表示照明估计器（Illumination Estimator），$R$ 表示失真恢复器（Corruption Restorer）。$E$ 以 $I$ 及其照明先验图 $L_p ∈ R^{H×W}$ 作为输入，其中 $L_p = mean_c(I)$，表示沿通道方向计算每个像素的平均值。
>补充：RGB三通道（红绿蓝）中，人眼感知颜色的敏感度排名是$G>R>B$。

 $E$ 输出亮化图像 $I_{lu}$ 和亮化特征 $F_{lu} ∈ R^{H×W×C}$。
 然后，将 $I_{lu}$ 和 $F_{lu}$ 输入 $R$，以恢复损坏并生成增强图像 $I_{en} ∈ R^{H×W×3}$。

**照明估计器（$\mathcal E$）的结构**如图 2(a)(i) 所示。
$\mathcal E$ 首先使用一个 $1×1$ 卷积将 $I$ 和 $L_p$ 的拼接结果进行融合。我们注意到，良好曝光的区域可以为欠曝光区域提供语义上下文信息。因此，采用一个**深度可分离卷积（Depth-wise Separable Conv 5×5）** 来建模不同光照区域间的交互，以生成亮化特征 $F_{lu}$。接着，$E$ 使用一个 $1×1$ 卷积来聚合 $F_{lu}$，以生成亮化映射 $\bar{L} ∈ R^{H×W×3}$。我们将 $\bar{L}$ 设为三通道 RGB 张量，而不是使用单通道映射，以提高其跨 RGB 通道建模非线性的能力，从而实现更好的颜色增强。然后，使用 $\bar{L}$ 在公式 (3) 中对 $I$ 进行亮化。

**讨论（Discussion）**
 (i) 与以往的 Retinex 深度学习方法不同，我们的 ORF 估计的是 $\bar{L}$ 而非照明图 $L$，因为如果 ORF 估计 $L$，则亮化图像需通过逐元素除法 $I ./ L$ 获得。计算机在处理极小值（甚至为0）的张量时容易出错，除法操作可能导致数据溢出。此外，计算机随机产生的微小误差也会被该除法操作放大，从而导致估计不准确。因此，建模 $\bar{L}$ 更为稳健。
 (ii) 先前的 Retinex 深度学习方法主要关注抑制反射图像上的噪声（即公式 (2) 中的 $\hat{R}$），但忽略了照明图估计误差（即公式 (2) 中的 $\hat{L}$），从而在亮化过程中容易产生欠曝光/过曝光和颜色失真。相比之下，我们的 ORF 同时考虑了这些损坏，并使用 $\mathcal R$ 来恢复它们。
  
------

### 3.2 照明引导 Transformer（Illumination-Guided Transformer）

先前的深度学习方法主要依赖于 CNN，在捕捉长距离依赖性方面存在局限性。一些 CNN-Transformer 混合方法，如 SNR-Net，由于全局多头自注意力（global MSA）的高计算复杂度，仅在 U 形 CNN 的最低分辨率上使用一个全局 Transformer 层。Transformer 在低光图像增强中的潜力尚未被充分挖掘。为弥补这一空白，我们设计了一个**照明引导 Transformer（IGT）**，用于充当公式 (5) 中的失真恢复器 $R$。
 
**网络结构（Network Structure）**
如图 2(a)(ii) 所示，IGT 采用三尺度的 U 形结构。IGT 的输入为亮化图像 $I_{lu}$。在下采样分支中，$I_{lu}$ 依次经过一个 $3×3$ 卷积、一个 IGAB、一个步幅为 2 的 $4×4$ 卷积（用于下采样）、两个 IGAB、再一个步幅为 2 的 $4×4$ 卷积，以生成分层特征 $F_i ∈ R^{H/2^i × W/2^i × 2^iC}$，其中 $i = 0,1,2$。然后，$F_2$ 经过两个 IGAB。接着，一个对称的上采样分支用于恢复特征分辨率，使用步幅为 2 的 $2×2$ 反卷积（deconv）进行上采样。跳跃连接（skip connection）用于缓解下采样造成的信息丢失。上采样分支输出一个残差图像 $I_{re} ∈ R^{H×W×3}$。最终，增强图像由 $I_{en} = I_{lu} + I_{re}$ 得到。

------

**IG-MSA（照明引导多头自注意力）**
如图 2(c) 所示，由 $\mathcal E$ 估计得到的亮化特征 $F_{lu} ∈ R^{H×W×C}$ 被输入到 IGT 的每个 IG-MSA 中。需要注意的是，图 2(c) 展示的是最大尺度的 IG-MSA，对于较小尺度，使用步幅为 2 的 $4×4$ 卷积将 $F_{lu}$下采样以匹配空间尺寸（图中省略此细节）。如前所述，全局 MSA 的计算成本极高，因此限制了 Transformer 在低光图像增强中的应用。为此，IG-MSA 将单通道特征图视为一个 token，并计算自注意力。

------

首先，输入特征 $F_{in} ∈ R^{H×W×C}$ 被重塑为 tokens $X ∈ R^{HW×C}$。然后，将 $X$ 分为 $k$ 个头：
$$
X = [X_1, X_2, ···, X_k] \tag{6}
$$
其中 $X_i ∈ R^{HW×d_k}, d_k = C/k, i = 1,2,···,k$。
 图 2(c) 中仅展示了 $k=1$ 的情况，为简化起见省略了一些细节。
 对于每个头 $i$，使用三个无偏置的全连接层将 $X_i$ 线性映射为查询 $Q_i$、键 $K_i$ 和值 $V_i$：
$$
Q_i = X_i W_{Q_i}^T, \quad K_i = X_i W_{K_i}^T, \quad V_i = X_i W_{V_i}^T, \tag{7}
$$
其中 $W_{Q_i}, W_{K_i}, W_{V_i} ∈ R^{d_k×d_k}$ 为可学习参数，$T$ 表示转置。我们注意到，同一图像的不同区域可能具有不同的光照条件。暗区通常具有更严重的损坏且更难恢复，而光照较好的区域可以提供语义上下文信息，帮助增强暗区。因此，我们利用亮化特征 $F_{lu}$（编码了照明信息及不同光照区域间的交互）来引导自注意力的计算。为了与 $X$ 的形状对齐，我们同样将 $F_{lu}$ 重塑为 $Y ∈ R^{HW×C}$，并将其分为 $k$ 个头：
$$
Y = [Y_1, Y_2, ···, Y_k], \tag{8}
$$
其中 $Y_i ∈ R^{HW×d_k}, i = 1,2,···,k。$
 则每个头的自注意力计算为：
$$
Attention(Q_i, K_i, V_i, Y_i) = (Y_i ⊙ V_i) \, softmax\!\left(\frac{K_i^T Q_i}{α_i}\right), \tag{9}
$$
其中 $α_i ∈ R^1$ 为可学习参数，用于自适应地缩放矩阵乘积。
 随后，将 $k$ 个头的结果拼接，通过一个全连接层，再加上可学习的位置编码 $P ∈ R^{HW×C}$，得到输出 tokens $X_{out} ∈ R^{HW×C}$。
 最后，将 $X_{out}$ 重塑为输出特征 $F_{out} ∈ R^{H×W×C}$。

------

**复杂度分析（Complexity Analysis）**
 我们分析得出，IG-MSA 的主要计算复杂度来自于公式 (9) 中的两个矩阵乘法：
$$
R^{d_k×HW} × R^{HW×d_k} \quad 和 \quad R^{HW×d_k} × R^{d_k×d_k}.
$$
因此，IG-MSA 的复杂度可以表示为：
$$
O(IG\text{-}MSA) = k · [d_k · (d_k · HW) + HW · (d_k · d_k)] = 2HWk d_k^2 = \frac{2HWC^2}{k}. \tag{10}
$$
而某些先前的 CNN-Transformer 方法（如 SNR-Net）使用的全局 MSA 的复杂度为：
$$
O(G\text{-}MSA) = 2(HW)^2 C. \tag{11}
$$
比较公式 (10) 与 (11) 可以看出，O(G-MSA) 对输入空间尺寸 $HW$ 呈平方关系，这种负担很大且限制了 Transformer 在低光图像增强中的应用。相比之下，O(IG-MSA) 对空间尺寸是线性的。这种显著较低的计算复杂度使得 IG-MSA 可以被嵌入到网络的每个基本单元 IGAB 中，从而进一步挖掘 Transformer 在低光图像增强中的潜力。