#### **1. 引言**

时间序列预测在许多实际应用中具有重要意义，如能源预测、交通流量预测、气象预测等。随着深度学习技术的进步，基于Transformer的模型在时间序列预测中表现出了强大的能力。然而，传统Transformer模型存在计算复杂度高、内存消耗大等问题，尤其在长序列预测中效果不佳。**PatchTST**模型通过创新性的设计，解决了这些问题，提出了**分块**（Patching）和**通道独立性**（Channel-independence）的解决方案，显著提高了时间序列预测的效率和精度。

#### **2. 模型架构**

PatchTST模型的核心在于其创新的**分块和嵌入**模块，结合了Transformer的强大功能，提升了时间序列长序列预测的准确性和效率。

##### **2.1 分块（Patching）设计**

在PatchTST中，时间序列首先被切分成多个**子序列块（patches）**。每个子序列块代表时间序列中的一段连续区间，而不是直接将单个时间步作为输入。假设每个时间序列的长度为 $L$，我们将其切分为长度为 $P$ 的子序列，并根据设定的**步长（stride）** SSS 决定每个块之间的重叠程度。分块操作的步骤如下：

1. **时间序列切分**：每个时间序列 $x$ 被切分成多个长度为 $P$ 的块，步长为 $S$，得到 $N = \left\lfloor \frac{L - P}{S} \right\rfloor + 1$个子序列块。
    

##### **2.2 嵌入（Embedding）步骤**

每个分块后的时间序列子序列会经过以下步骤进行嵌入：

1. **实例标准化（Instance Normalization）**：  
    每个子序列块会先经过标准化处理，使其均值为0，标准差为1，从而减少不同序列之间的分布差异，避免训练过程中可能出现的梯度消失问题。
    
    $$x_i' = \frac{x_i - \mu(x_i)}{\sigma(x_i)}$$
    
    其中 $\mu(x_i)$ 和 $\sigma(x_i)$分别是子序列块 $x_i$ 的均值和标准差。
    
2. **位置编码（Position Embedding）**：  
    每个子序列块会添加**位置编码**，确保模型能够识别时间序列中各个时间步的位置。位置编码的公式为：
    
    $$x_i'' = x_i' + E_{\text{pos}}(i)$$
    
    其中 $E_{\text{pos}}(i)$是第 iii 个块的**位置编码**，$x_i'$ 是标准化后的子序列块。
    
3. **线性映射（Linear Projection）**：  
    每个子序列块经过标准化和位置编码后，通过线性层进行映射，将每个子序列块嵌入到更高维的空间中。线性映射公式为：
    
    $$x_i^{\text{emb}} = W_{\text{proj}} \cdot x_i''$$
    
    其中，$W_{\text{proj}}$ 是可学习的投影矩阵，$x_i^{\text{emb}}$ 是嵌入后的子序列块。
    
4. **构建输入Tokens**：  
    所有嵌入后的子序列块 $x_1^{\text{emb}}, x_2^{\text{emb}}, \dots, x_N^{\text{emb}}$​ 作为输入tokens传入Transformer模型进行后续处理。
    

##### **2.3 Transformer编码器（Transformer Encoder）**

Transformer的编码器接收经过嵌入后的子序列块作为输入，使用自注意力机制（Self-Attention）捕捉时间序列中各个子序列块之间的长程依赖关系。模型使用**稀疏自注意力**机制来减少计算开销和内存消耗，确保在长序列预测中不会受到计算复杂度的制约。

##### **2.4 最终预测**

Transformer处理完这些tokens后，输出的特征会通过**线性映射层**生成最终的时间序列预测结果：

$$\hat{y}_i = W_{\text{out}} \cdot \text{Transformer Output}$$

其中 WoutW_{\text{out}}Wout​ 是学习到的权重矩阵，用于从Transformer的输出生成最终的预测结果。

#### **3. 核心优势**

##### **3.1 高效处理长时间序列**

通过分块操作，PatchTST显著减少了输入的tokens数量，这在处理长时间序列时减少了计算和内存消耗，使得模型能够处理更长的历史数据。

##### **3.2 通道独立性**

通过**通道独立**的设计，PatchTST避免了多个时间序列通道之间的干扰，能够有效地处理多变量时间序列问题，尤其是在通道数量较多时，表现尤为突出。

##### **3.3 自监督学习能力**

PatchTST不仅能进行监督学习，还能通过自监督学习进行预训练，进而提升在不同数据集上的泛化能力。通过掩蔽输入序列中的部分块，模型被训练去预测这些掩蔽的部分，从而学习到更有意义的特征。

#### **4. 实验结果**

PatchTST在多个公开数据集上的实验结果表明，该模型显著优于传统的Transformer模型，如**Informer**和**Autoformer**。特别是在长时间序列预测任务中，PatchTST通过其分块和嵌入设计大幅减少了计算复杂度，并提高了预测精度。

#### **5. 总结**

PatchTST通过创新的分块和嵌入设计，显著提升了时间序列预测任务中的表现。分块设计使得模型能够处理更长的时间序列，减少了计算资源消耗，而通道独立性设计确保了多变量时间序列中各通道的独立处理，从而提高了模型的泛化能力。结合自监督学习，PatchTST能够进一步提升预测精度，并在多个基准数据集上超过了现有的SOTA模型。